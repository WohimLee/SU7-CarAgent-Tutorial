##  数据生产流程

### 0. 先给个整体结构

对「生成出来的数据」（例如 QA 对、意图识别样本、对话数据等），可以设计一条标准流水线：

1. 结构校验（Schema Validation）: JSON / 字段 / 类型 / 枚举值是否合法
2. 规则+模型双重打分（Scoring）:
- 规则打分（长度、关键词、模板规则）
- 模型打分（LLM 评价、NLI、RAG 质量度量）
3. 一致性 & 逻辑检查（Consistency Check）: 自洽性、自反问答、跨样本冲突
4. 相似度去重 & 覆盖度控制（Dedup & Coverage）: embedding 聚类 / MinHash / SimHash
5. 风险与合规过滤（Safety / Compliance）: 敏感内容、泄露风险、品牌/法律风险
6. 抽样人工 QA（Human-in-the-loop）: 动态阈值调优、反馈回流
7. 质量标签与数据版本化（DataOps）: 记录每条样本的多维指标 & 审核状态，做 AB 实验

下面详细展开每一块，尽量偏「可落地」一点。

### 1. 结构校验：先把垃圾挡在门外

这一层完全可以做成 纯工程化的硬门槛，不需要 LLM 参与：

#### 1.1 Schema & 业务规则校验

例如针对 QA 数据（以 JSON 为例）：
```
{
  "query": "用户的问题",
  "answer": "模型生成的回答",
  "intent": "绑定的意图标签",
  "source": "生成/挖掘方式",
  "meta": { ... }
}
```

可以做的检查：

- 字段完整性：缺失 query/answer 直接丢弃
- 字段类型：query answer 必须是字符串、长度允许范围
- 业务规则：
    - query 长度 5~128 字
    - answer 长度 10~1024 字
    - answer 不能只包含 URL、emoji、标点
    - 不能是「对不起，我是大语言模型」这种模板语

这些全部用普通代码即可。

### 2. 规则+模型双重打分：质量「量化」是前提
#### 2.1 规则打分（cheap 但稳）

可以给每条样本打几个简单分：

##### 覆盖度分：

- 问题中是否包含关键业务词（产品名、功能名）
- 标签/意图是否在预设集合

##### 可读性/复杂度分：

- token 数太少/太多都扣分
- 标点比例、数字比例过高 → 垃圾嫌疑

##### 模板/废话检测：

- 用一批正则 / 关键词过滤：
    - “我是 AI 模型…”
    - “如需更多帮助，请咨询专业人员”
    - “抱歉，由于我是语言模型，无法…”
- 出现则标记为 low_quality_reason: boilerplate

可以汇总成一个 `rule_score ∈ [0,1]`。

#### 2.2 LLM 评分（G-Eval / LLM-as-a-judge 思路）

对 QA 对、意图识别样本，用一个「比生产模型更强或等价」的 LLM 来打分：

##### 典型打分维度：

- 正确性 / 准确性（是否回答了问题？是否有事实错误？）
- 相关性（是否跑题？）
- 完整性 / 细致程度
- 语言流畅度 / 可读性
- 安全/合规

给个 prompt 模板示例（伪代码）：
```py
你是一个数据质检员。给定一个用户问题和系统回答，请从以下维度打分：
1. 是否回答了问题（0-5）
2. 回答是否事实正确（0-5）
3. 回答是否与问题高度相关（0-5）
4. 表达是否清晰流畅（0-5）
5. 是否包含不当内容（0-5，0=安全，5=严重违规）

请输出 JSON，仅包含这些字段：
{
  "answer_relevance": 0-5,
  "answer_correctness": 0-5,
  "fluency": 0-5,
  "safety": 0-5,
  "comment": "简短说明"
}
```

##### 实践建议：

- 不要一次性给几万条都让 LLM 打分，先抽样看分布，确定维度设计合理再放量。
- 大规模时可以：高风险/高价值样本用贵模型，普通样本用便宜模型或规则。

### 3. 一致性 & 逻辑检查：防止「一本正经地胡说八道」
#### 3.1 自洽性（Self-consistency）

对每个 QA 对，可以：

- 让 LLM 再回答同一个 query，多次采样
- 比较生成回答之间的 语义相似度：如果多轮回答差异巨大，说明问题/数据不稳定，可标记为 low_consistency
- 或者更简单：
    - Prompt LLM：给定问题 Q 和回答 A，请判断 A 是否逻辑自洽，有无自相矛盾或明显逻辑错误。

#### 3.2 反向验证（逆问 / NLI）

两种常见技巧：

- 问答互证
    - 用 LLM 让它根据 answer 重新生成一个问题 q'，
    - 看 q' 与原始 query 的相似度
- 自然语言推理（NLI）
    - 把 answer 拆成几个事实句子，让 LLM 判断是否互相矛盾
    - 或者用专门的 NLI 模型（entailment / contradiction / neutral）

### 4. 去重 & 覆盖度：防止数据「一千条长得一样」

企业环境下，生成量很大，很容易：

同一个问题/答案被重复生成

只是换个同义词，造成 语义高度重复

4.1 基于 embedding 的相似度去重

标准做法：

用一个稳定的文本向量模型（open source 或内部 embedding 服务）：

对 query 或 query + answer 拼接成一条文本做 embedding

使用：

余弦相似度 + 阈值 做近似去重

或者用 FAISS / Milvus / PGVector 做 ANN 检索

对于相似度 > 阈值（如 0.9）的样本：

保留打分高 / 来源更可信 / 更新的那条

其他打上 dedup_by_id，不进入训练或评估集

4.2 基于 MinHash / SimHash 的文本去重

对于大量短文本（意图识别样本等），可以：

先做文本规整（全角半角、大小写、去停用词）

用 SimHash 生成 hash，按 Hamming 距离做近似去重

好处是：成本极低，适合「几千万条 log」快速粗去重

4.3 覆盖度与多样性控制

除了去重，还要避免「全是一个 intent」：

统计维度：

不同意图/标签下的样本量分布

不同领域/产品/功能的覆盖情况

问题长度分布、类型分布（操作指令、咨询、投诉、寒暄…）

做「分层抽样」：

对样本池按意图/领域聚类后，每个簇仅抽一定比例进入最终训练集

避免头部意图撑满整个数据集

5. RAG 场景：专门的「检索+回答质量」评分

你提到 RAG，那生成出来的 QA 或回答往往依赖于知识库/文档，这里有一些专用指标可以嵌入流水线：

5.1 检索质量打分

对每条样本，除了 query 和 answer，还要保留：

检索到的 top-k 文档片段（context）

文档 ID / 源文件 / 版本号

打分维度：

检索相关性：

LLM 或小模型判断「context 是否与 query 高度相关」

证据覆盖度：

问题中关键实体/属性是否能在 context 中找到对应

5.2 回答与证据的一致性

典型做法：

Prompt LLM：

给定问题 Q、回答 A，以及知识片段 C，请判断 A 中的关键信息是否能在 C 中找到依据。
输出：support_score (0-5)，以及是否出现「编造/幻觉」。

如果 support_score 很低但 answer 说得头头是道，就标记为潜在「幻觉样本」，降权或丢弃。

6. 风险与合规过滤（企业场景里非常关键）

企业级环境下，这一步通常不能省，而且往往会接上 安全团队/法务团队 的规则库：

敏感内容：

政治、暴力、黄赌毒、极端言论

法务相关：

是否出现未经授权的客户信息、手机号、身份证、邮箱等 PII

是否提到竞品攻击、违规承诺（比如医疗、金融承诺）

做法：

Rule-based：

黑名单词 + 正则（手机号/身份证/email）

模型-based：

用专门的安全分类模型或 LLM 来标注：

normal / sensitive / illegal / compliance_risk 等

打上标签后，可以：

敏感/高风险：直接丢弃或需要人工审核

正常：进入后续环节

7. 人在回路（Human-in-the-loop）：闭环优化

再聪明的自动化流水线，也必须配一个「抽样人工查验」来：

校正阈值（例如：LLM_score > 3.5 就入库，合理吗？）

发现自动规则捕捉不到的问题

给不同打分维度「真实含义」贴上直觉判断

实践建议：

分层抽样：

高分、中分、低分各抽一点

不同意图/领域都要覆盖

人工打真实标签：

good / usable / bad

并记录「坏的」原因：跑题、事实错误、表达垃圾、风险内容…

迭代规则：

针对坏样本逆向分析：是规则漏了？还是 LLM-judge 的 prompt 不好？

调整完规则后再跑一轮，逐步收敛。

8. 数据标注：给每条样本打「多维质量元数据」

企业级环境，数据不仅仅是「一堆文本」，而是要支持后续：

模型训练时做「加权采样 / curriculum learning」

做不同版本模型的对比（A/B 实验）

出问题时能追溯「哪批数据写坏了模型」

所以推荐给每条样本存一套 metadata，比如：

{
  "id": "uuid",
  "query": "...",
  "answer": "...",
  "intent": "xxx",
  "source": "log_mining / synthetic / manual",
  "rule_score": 0.83,
  "llm_quality_score": 4.2,
  "rag_support_score": 3.9,
  "consistency_score": 0.75,
  "safety_label": "normal",
  "dedup_group_id": "hash123",
  "human_label": "good / bad / not_checked",
  "version": "2025-11-rag-v3",
  "created_at": "...",
  "approved": true
}


这样后续训练时可以：

只用 approved == true 且 llm_quality_score > 某阈值 的数据

对 rule_score/rag_support_score 高的样本加大采样权重

对负样本（比如 bad）专门训练一个「拒答/拒绝幻觉」子模块

9. 工程落地的小建议（企业级环境相关）
9.1 做成「离线批处理流水线」

典型技术栈：

数据流：Kafka / object storage（S3，OSS 等）

计算：Spark/Flink/内部批处理平台

各种打分服务：

embedding 服务

LLM 评分服务（内部 HTTP API）

安全分类服务

把整条流水线配置成：

raw_generated_data → validate → rule_score → model_score → dedup → safety_filter → sampling + human_QA → curated_dataset

9.2 和 MLOps / DataOps 打通

用特定的表/仓库存放「候选数据」与「已审核数据」

每次模型训练要明确「用的是哪一批数据（版本号）」

线上监控模型表现（例如 FAQ 命中率、工单转人工率）和「当前数据版本」挂钩

如果上线后效果变差，能迅速定位到是哪次数据更新有问题

10. 如果你要快速起步，可先做一个「简化版」

如果现在是 PoC/早期阶段，可以先做一条 轻量级管线：

Schema + 基础规则过滤

LLM-judge 给 QA 对打 3 个分：相关性、正确性、安全

基于 query 的 embedding 去重（相似度>0.9 去掉）

对每个意图/领域做分层抽样 + 人工 QA

只保留「人工看过、质量 ok」的子集作为第一批高质量数据

等这套小流程跑顺畅了，再逐步：

加上 RAG 支持度打分

加上自洽性检查

加上更细粒度的风险标签